{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ccdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import T5TTS_Model\n",
    "from nemo.collections.tts.data.text_to_speech_dataset import T5TTSDataset, DatasetSample\n",
    "from omegaconf.omegaconf import OmegaConf, open_dict\n",
    "import torch\n",
    "import os\n",
    "import soundfile as sf\n",
    "from IPython.display import display, Audio\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5798ac",
   "metadata": {},
   "source": [
    "### Checkpoint Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04445f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint and Hparams Paths\n",
    "hparams_file = \"/datap/misc/Experiments/SimpleT5Explore/LocalTraining_LRH/T5TTS/0/hparams.yaml\"\n",
    "checkpoint_file = \"/datap/misc/Experiments/SimpleT5Explore/LocalTraining_LRH/T5TTS/0/checkpoints/test.ckpt\"\n",
    "codecmodel_path = \"/datap/misc/checkpoints/AudioCodec_21Hz_no_eliz.nemo\"\n",
    "\n",
    "# Temp out dir for saving audios\n",
    "out_dir = \"/datap/misc/t5tts_inference_notebook_samples\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf2a16",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = OmegaConf.load(hparams_file).cfg\n",
    "\n",
    "with open_dict(model_cfg):\n",
    "    model_cfg.codecmodel_path = codecmodel_path\n",
    "    if hasattr(model_cfg, 'text_tokenizer'):\n",
    "        # Backward compatibility for models trained with absolute paths in text_tokenizer\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_dict = \"scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt\"\n",
    "        model_cfg.text_tokenizer.g2p.heteronyms = \"scripts/tts_dataset_files/heteronyms-052722\"\n",
    "        model_cfg.text_tokenizer.g2p.phoneme_probability = 1.0\n",
    "    model_cfg.train_ds = None\n",
    "    model_cfg.validation_ds = None\n",
    "\n",
    "\n",
    "model = T5TTS_Model(cfg=model_cfg)\n",
    "print(\"Loading weights from checkpoint\")\n",
    "ckpt = torch.load(checkpoint_file)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "print(\"Loaded weights.\")\n",
    "\n",
    "model.use_kv_cache_for_inference = True\n",
    "\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b5711",
   "metadata": {},
   "source": [
    "### Initialize Dataset class and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = T5TTSDataset(\n",
    "    dataset_meta={},\n",
    "    sample_rate=model_cfg.sample_rate,\n",
    "    min_duration=0.5,\n",
    "    max_duration=20,\n",
    "    codec_model_downsample_factor=model_cfg.codec_model_downsample_factor,\n",
    "    bos_id=model.bos_id,\n",
    "    eos_id=model.eos_id,\n",
    "    context_audio_bos_id=model.context_audio_bos_id,\n",
    "    context_audio_eos_id=model.context_audio_eos_id,\n",
    "    audio_bos_id=model.audio_bos_id,\n",
    "    audio_eos_id=model.audio_eos_id,\n",
    "    num_audio_codebooks=model_cfg.num_audio_codebooks,\n",
    "    prior_scaling_factor=None,\n",
    "    load_cached_codes_if_available=True,\n",
    "    dataset_type='test',\n",
    "    tokenizer_config=None,\n",
    "    load_16khz_audio=model.model_type == 'single_encoder_sv_tts',\n",
    "    use_text_conditioning_tokenizer=model.use_text_conditioning_encoder,\n",
    "    pad_context_text_to_max_duration=model.pad_context_text_to_max_duration,\n",
    "    context_duration_min=model.cfg.get('context_duration_min', 5.0),\n",
    "    context_duration_max=model.cfg.get('context_duration_max', 5.0),\n",
    ")\n",
    "test_dataset.text_tokenizer, test_dataset.text_conditioning_tokenizer = model._setup_tokenizers(model.cfg, mode='test')\n",
    "\n",
    "\n",
    "\n",
    "def get_audio_duration(file_path):\n",
    "    with sf.SoundFile(file_path) as audio_file:\n",
    "        # Calculate the duration\n",
    "        duration = len(audio_file) / audio_file.samplerate\n",
    "        return duration\n",
    "\n",
    "def create_record(text, context_audio_filepath=None, context_text=None):\n",
    "    dummy_audio_fp = os.path.join(out_dir, \"dummy_audio.wav\")\n",
    "    dummy_audio = sf.write(dummy_audio_fp, np.zeros(22050 * 3), 22050)  # 3 seconds of silence\n",
    "    record = {\n",
    "        'audio_filepath' : dummy_audio_fp,\n",
    "        'duration': 3.0,\n",
    "        'text': text,\n",
    "        'speaker': \"dummy\",\n",
    "    }\n",
    "    if context_text is not None:\n",
    "        assert context_audio_filepath is None\n",
    "        record['context_text'] = context_text\n",
    "    else:\n",
    "        assert context_audio_filepath is not None\n",
    "        record['context_audio_filepath'] = context_audio_filepath\n",
    "        record['context_audio_duration'] = get_audio_duration(context_audio_filepath)\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa7a5a",
   "metadata": {},
   "source": [
    "### Set transcript and context pairs to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7374d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change sample text and prompt audio/text here\n",
    "audio_base_dir = \"/\"\n",
    "test_entries = [\n",
    "    create_record(\n",
    "        text=\"This is a sample sentence to test the speed of my text to speech synthesis model. Call nine eight nine, two four seven, nine nine eight three.\",\n",
    "        context_audio_filepath=\"/datap/misc/Datasets/riva/Lindy/22khz/WIZWIKI/LINDY_WIZWIKI_001254.wav\", # Supply either context_audio_filepath or context_text, not both\n",
    "#         context_text=\"Speaker and Emotion: | Language:en Dataset:Riva Speaker:Lindy_WIZWIKI |\",        \n",
    "    ),\n",
    "]\n",
    "\n",
    "data_samples = []\n",
    "for entry in test_entries:\n",
    "    dataset_sample = DatasetSample(\n",
    "        dataset_name=\"sample\",\n",
    "        manifest_entry=entry,\n",
    "        audio_dir=audio_base_dir,\n",
    "        feature_dir=audio_base_dir,\n",
    "        text=entry['text'],\n",
    "        speaker=None,\n",
    "        speaker_index=0,\n",
    "        tokenizer_names=[\"english_phoneme\"], # Change this for multilingual: \"english_phoneme\", \"spanish_phoneme\", \"english_chartokenizer\", \"german_chartokenizer\".. \n",
    "    )\n",
    "    data_samples.append(dataset_sample)\n",
    "    \n",
    "test_dataset.data_samples = data_samples\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=test_dataset.collate_fn,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9866b",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_idx = 0\n",
    "for bidx, batch in enumerate(test_data_loader):\n",
    "    print(\"Processing batch {} out of {}\".format(bidx, len(test_data_loader)))\n",
    "    model.t5_decoder.reset_cache(use_cache=True)\n",
    "    batch_cuda ={}\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], torch.Tensor):\n",
    "            batch_cuda[key] = batch[key].cuda()\n",
    "        else:\n",
    "            batch_cuda[key] = batch[key]\n",
    "    import time\n",
    "    st = time.time()\n",
    "    predicted_audio, predicted_audio_lens, _, _ = model.infer_batch(\n",
    "        batch_cuda, \n",
    "        max_decoder_steps=500, \n",
    "        temperature=0.6, \n",
    "        topk=80, \n",
    "        use_cfg=True, \n",
    "        cfg_scale=1.8\n",
    "    )\n",
    "    print(\"generation time\", time.time() - st)\n",
    "    for idx in range(predicted_audio.size(0)):\n",
    "        predicted_audio_np = predicted_audio[idx].float().detach().cpu().numpy()\n",
    "        predicted_audio_np = predicted_audio_np[:predicted_audio_lens[idx]]\n",
    "        audio_path = os.path.join(out_dir, f\"predicted_audio_{item_idx}.wav\")\n",
    "        sf.write(audio_path, predicted_audio_np, model.cfg.sample_rate)\n",
    "        display(Audio(audio_path))\n",
    "        item_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
